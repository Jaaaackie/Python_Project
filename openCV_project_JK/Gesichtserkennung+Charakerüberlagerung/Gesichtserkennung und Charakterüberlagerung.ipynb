{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43907191",
   "metadata": {},
   "source": [
    "# Gesichtserkennung und Charakterüberlagerung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736ef2bc",
   "metadata": {},
   "source": [
    "## Gesichtserkennung (Face Detection) vs. Gesichtsidentifikation (Face Recognition)\n",
    "- Gesichtserkennung : Erkennen von Gesichtsmerkmalen in Bildern oder Videos, um Gesichter zu finden\n",
    "    - Beispiel : Temperaturmessung beim Betreten eines Gebäudes\n",
    "- Gesichtsidentifikation : Bestimmen, wer die erkannten Gesichter sind\n",
    "    - Beispiel : Anwesenheitskontrolle in Vorlesungen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9d289e-852a-4976-9e4f-36dbd5cb9767",
   "metadata": {},
   "source": [
    "Dieses Projekt erzielt das besseres Verständnis. Die unten stehenden Codes basieren auf die Referenz :  https://github.com/google/mediapipe/blob/master/docs/solutions/face_detection.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39d6c0c",
   "metadata": {},
   "source": [
    "### Package-Installation (anaconda prompt)\n",
    "> pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a7a91b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Variablen für das Finden und Markieren von Gesichtern definieren\n",
    "mp_face_detection = mp.solutions.face_detection # Modul \"face_detection\" für Gesichtserkennung\n",
    "mp_drawing = mp.solutions.drawing_utils # Modul \"drawing_utils\" zur Markierung der Gesichtsmerkmale\n",
    "\n",
    "# Videodatei öffnen\n",
    "cap = cv2.VideoCapture('face_video.mp4')\n",
    "\n",
    "# Die with-Anweisung ermöglicht es, Dateien zu öffnen, ohne sie explizit mit close() schließen zu müssen\n",
    "# Die automatische Freigabe der Ressourcen ist ein zusätzlicher Vorteil\n",
    "with mp_face_detection.FaceDetection(model_selection = 0, min_detection_confidence = 0.7) as face_detection:\n",
    "    # model_selection = 0 (Gesichter in 2m Entfernung) / 1 (Gesichter in 5m Entfernung)\n",
    "    # min_detection_confidence : Mindestvertrauenswürdigkeit, um ein Gesicht zu erkennen (wie Threshold)\n",
    "    \n",
    "    while cap.isOpened(): # überprüfen, ob das Video erfolgreich geöffnet wurde\n",
    "        success, image = cap.read()\n",
    "    \n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        # Zur Leistungsoptimierung kann das Bild optional als nicht schreibbar markert werden, um es per Referenz zu übergeben\n",
    "        image.flags.writeable = False\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # mediapipe verwendet RGB-Farbformat\n",
    "        results = face_detection.process(image) # Gesichter im Bild erkennen und es in \"results\" speichern\n",
    "\n",
    "        # Die Gesichtserkennung im Bild zeichnen\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        if results.detections: # durch while-Schleife erkannte Gesichter im Bild zeichnen\n",
    "            # 6 relevante Gesichtsmerkmale : rechtes Auge, linkes Auge, Nasenspitze, Mundmitte, rechtes Ohr, linkes Ohr\n",
    "            for detection in results.detections:\n",
    "                # mp_drawing.draw_detection(image, detection)\n",
    "                # print(detection) # Informationen zu den in einem einzelen Frame erkannten Objekten\n",
    "                \n",
    "                # bestimmte Koordinaten abrufen\n",
    "                keypoints = detection.location_data.relative_keypoints\n",
    "                right_eye = keypoints[0]\n",
    "                left_eye = keypoints[1]\n",
    "                nose_tip = keypoints[2]\n",
    "                \n",
    "                h, w, _ = image.shape # Bildgröße (height, width, channel), Channel ist aber jetzt nicht gebraucht\n",
    "                \n",
    "                # Koordinaten der Gesichtsmerkmale im Bild (x, y)\n",
    "                right_eye = (int(right_eye.x * w), int(right_eye.y * h)) \n",
    "                left_eye =  (int(left_eye.x * w), int(left_eye.y * h))\n",
    "                nose_tip = (int(nose_tip.x * w), int(nose_tip.y * h))\n",
    "                \n",
    "                # Kreise um die Augen\n",
    "                cv2.circle(image, right_eye, 50, (255, 0, 0), 10, cv2.LINE_AA) # blau\n",
    "                cv2.circle(image, left_eye, 50, (0, 255, 0), 10, cv2.LINE_AA) # grün\n",
    "                \n",
    "                # Kreise um die Nase\n",
    "                cv2.circle(image, nose_tip, 10, (0, 255, 255), 10, cv2.LINE_AA) # gelb\n",
    "                \n",
    "        cv2.imshow('MediaPipe Gesichtserkennung', cv2.resize(image, None, fx = 0.5, fy = 0.5))\n",
    "\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a887c2c",
   "metadata": {},
   "source": [
    "## Bilderüberlagerung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "403aed22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "cap = cv2.VideoCapture('face_video.mp4')\n",
    "\n",
    "# Bilder laden\n",
    "image_right_eye = cv2.imread('right_eye.png') # 100 x 100\n",
    "image_left_eye = cv2.imread('left_eye.png') # 100 x 100\n",
    "image_nose = cv2.imread('nose.png') # 300 x 100 (width, height)\n",
    "\n",
    "with mp_face_detection.FaceDetection(model_selection = 0, min_detection_confidence = 0.9) as face_detection:\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "    \n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        image.flags.writeable = False\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = face_detection.process(image)\n",
    "\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        if results.detections:\n",
    "            for detection in results.detections:\n",
    "                \n",
    "                keypoints = detection.location_data.relative_keypoints\n",
    "                right_eye = keypoints[0]\n",
    "                left_eye = keypoints[1]\n",
    "                nose_tip = keypoints[2]\n",
    "                \n",
    "                h, w, _ = image.shape\n",
    "                right_eye = (int(right_eye.x * w) - 20, int(right_eye.y * h) - 100) # Anpassen der Bildposition der Ohren\n",
    "                left_eye =  (int(left_eye.x * w) - 20, int(left_eye.y * h) - 100)\n",
    "                nose_tip = (int(nose_tip.x * w), int(nose_tip.y * h))\n",
    "                \n",
    "                # Bilder auf die jewieiligen Merkmale zeichnen\n",
    "                image[right_eye[1] - 50 : right_eye[1] + 50, right_eye[0] - 50 : right_eye[0] + 50] = image_right_eye\n",
    "                # Die Größe von \"image_right_eye\" beträgt 100 x 100,\n",
    "                # und die Koordinaten von \"right_eye\" sind der Mittelpunkt des Auges,\n",
    "                # daher wird der Bereich 50 Pixel darüber und darunter sowie 50 Pixel links und rechts davon (Viereck)ausgewählt\n",
    "                image[left_eye[1] - 50 : left_eye[1] + 50, left_eye[0] - 50 : left_eye[0] + 50] = image_left_eye\n",
    "                # Wenn man eine echte \"solution\" entwickeln würde, müsste man die Größe von \"image_left_eye_ mit \"image_left_eye.shape()\" herausfinden\n",
    "                image[nose_tip[1] - 50 : nose_tip[1] + 50, nose_tip[0] - 150 : nose_tip[0] + 150] = image_nose\n",
    "                \n",
    "                \n",
    "        cv2.imshow('MediaPipe Charakterueberlagerung', cv2.resize(image, None, fx = 0.5, fy = 0.5))\n",
    "\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95658aac",
   "metadata": {},
   "source": [
    "Kostenlose Online-Bildbearbeitungswerkzeuge: https://pixlr.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae12c88c",
   "metadata": {},
   "source": [
    "- image = cv2.imread('file_name', cv2.IMREAD_UNCHANGED)\n",
    "    - Mit cv2.imread('file_name') allein erhält man nur width, height, channel. Um die Transparenz ebenfalls zu übernehmen, muss die Option \"cv2.IMREAD_UNCHANGED verwendet werden.\n",
    "\n",
    "- Fehler : could not broadcast (100, 100, 4) into (100, 100, 3)\n",
    "    - Ein 4-Kanal-Bild kann nicht in ein 3-Kanal-Bild eingefügt werden.\n",
    "        - Man muss die benötigten Kanäle separat extrahieren und einfügen\n",
    "        - Eine \"overlay()\"-Funktion erstellen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dca15c",
   "metadata": {},
   "source": [
    "#### overlay()-Funktion erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3996dfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# ausführliche Erklärung zu \"overlay()\"-Funktion steht unten\n",
    "def overlay(image, x, y, w, h, overlay_image): # Parameter: (Zielimage, x, y, width, height, Überlagerungsbild)\n",
    "    alpha = overlay_image[:, :, 3] #BGRA\n",
    "    mask_image = alpha / 255\n",
    "\n",
    "    for c in range(0, 3): # channel BGR\n",
    "        image[y - h : y + h, x - w : x + w, c] = (overlay_image[:, :, c] * mask_image) + (image[y - h : y + h, x - w : x + w, c] * (1 - mask_image))\n",
    "        \n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "cap = cv2.VideoCapture('face_video.mp4')\n",
    "\n",
    "image_right_eye = cv2.imread('right_eye_cat.png', cv2.IMREAD_UNCHANGED)\n",
    "image_left_eye = cv2.imread('left_eye_cat.png', cv2.IMREAD_UNCHANGED)\n",
    "image_nose = cv2.imread('nose_cat.png', cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "with mp_face_detection.FaceDetection(model_selection = 0, min_detection_confidence = 0.9) as face_detection:\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "    \n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        image.flags.writeable = False\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = face_detection.process(image)\n",
    "\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        if results.detections:\n",
    "            for detection in results.detections:\n",
    "                \n",
    "                keypoints = detection.location_data.relative_keypoints\n",
    "                right_eye = keypoints[0]\n",
    "                left_eye = keypoints[1]\n",
    "                nose_tip = keypoints[2]\n",
    "                \n",
    "                h, w, _ = image.shape\n",
    "                right_eye = (int(right_eye.x * w) - 20, int(right_eye.y * h) - 100)\n",
    "                left_eye =  (int(left_eye.x * w) - 20, int(left_eye.y * h) - 100)\n",
    "                nose_tip = (int(nose_tip.x * w), int(nose_tip.y * h))\n",
    "                \n",
    "                # overlay(image, x, y, w, h, overlay_image)\n",
    "                overlay(image, *right_eye, 50, 50, image_right_eye)\n",
    "                overlay(image, *left_eye, 50, 50, image_left_eye)\n",
    "                overlay(image, *nose_tip, 150, 50, image_nose)\n",
    "                \n",
    "                \n",
    "        cv2.imshow('MediaPipe verbesserte Charakterueberlagerung', cv2.resize(image, None, fx = 0.5, fy = 0.5))\n",
    "\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3755713c",
   "metadata": {},
   "source": [
    "#### Erklärung zu overlay_image (Überlagerungsbild)\n",
    "- Das Überlagerungsbild (overlay_image) : 4 Kanäle (BGRA)\n",
    "- Das Zielimage : 3 Kanäle (BGR), ohne alpha-Kanal für die Transparenz\n",
    "- Das Ergebnis-Image : ddas Überlagerungsbild auf das Zielimage angemessen überlappen, um ein 3-Kanal-Bild zu erstellen\n",
    "\n",
    "Wie funktioniert das?\n",
    "- nur den alpha-Kanal (index = 3) aus dem Überlagerungsbild extrahieren [:, :, 3]\n",
    "    - Der alpha-Wert liegt im Bereich 0 - 255 wie BGR\n",
    "    - alpha / 255 ergibt mask_image mit Werten zwischen 0.0 und 1.0 (mask_image : Transparenz alpha)\n",
    "    - 1 - mask_image ergibt die invertierte Transparenz, ebenfalls zwischen 1.0 - 0.0\n",
    "    - mask_image und 1 - mask_image sind zueinander komplementär (mask_image + (1 - mask_image) = 1)\n",
    "        - 1 : vollständig undurchsichtig\n",
    "        - 0 : vollständig transparent\n",
    "- Wenn das Überlagerungsbild 4-Kanal-BGRA ist und der alpha-Wert 1 ist, bedeutet das, dass es vollständig undurchsichtig ist und die Transparenz keine Rolle spielt\n",
    "    - Das resultierende Bild wäre dann ebenfalls ein vollständig undurchsichtiges 3-Kanal-Bild, ohne alpha-Wert\n",
    "    \n",
    "Berechnung\n",
    "- 3-Kanal-Ergebnis-Image BGR = (BGR aus 4-Kanal-Überlagerungsbild * mask_image) + (3-Kanal-Zielimage BGR * (1 - mask_image))\n",
    "    - nur die BGR aus dem 4-Kanal-Überlagerungsbild verwenden\n",
    "        - BGR aus dem 4-Kanal-Überlagerungsbild * mask_image\n",
    "            - BGR * Transparenz\n",
    "    - 3-Kanal-Zielimage BGR * (1 - mask_image)\n",
    "        - Also, 3-Kanl-Zielimage BGR * (Wert zwischen 0 und 1)\n",
    "            - BGR * invertierte Transparenz\n",
    "- Beispiel 1 :\n",
    "    - Überlagerungsbild 4-Kanal BGRA : Gelb, mask_image = 1.0 (vollständig undurchsichtiges Gelb)\n",
    "        - (0, 255, 255) * 1.0 = (0, 255, 255) : gelbes Bilddatei unverändert\n",
    "    - Zielimage 3-Kanal BGR : Rot\n",
    "        - (0, 0, 255) * (1 - 1.0) = (0, 0, 255) * 0.0 = 0.0 : nicht-existiertes Datei\n",
    "    - Ergebnis-Image BGR durch Berechnung: (0, 255, 255) + (0, 0, 0) = (0, 255, 255), Gelb, also Überlagerungsbild unverändert\n",
    "\n",
    "- Beispiel 2 :\n",
    "    - Überlagerungsbild 4-Kanal BGRA : Gelb, mask_image = 0.0 (vollständig transparentes Gelb)\n",
    "        - (0, 255, 255) * 0.0 = 0.0\n",
    "    - Zielimage 3-Kanal BGR : Rot\n",
    "        - (0, 0, 255) * (1 - 0.0) = (0, 0, 255) * 1.0 = (0, 0, 255)\n",
    "    - Ergebnis-Image BGR : (0, 0, 0) + (0, 0, 255) = (0, 0, 255), Rot, also Zielimage unverändert\n",
    "    \n",
    "- Beispiel 3 :\n",
    "    - Überlagerungsbild 4-Kanal BGRA : Gelb, mask_image = 0.4 (transluzentes Gelb)\n",
    "        - (0, 255, 255) * 0.4 = (0, 102, 102)\n",
    "    - Zielimage 3-Kanal BGR : Rot\n",
    "        - (0, 0, 255) * (1 - 0.4) = (0, 0, 255) * 0.6 = (0, 0, 153)\n",
    "    - Ergebnis-Image BGR : (0, 102, 102) + (0, 0, 153) = (0, 102, 255), Orange, also Mischung aus Überlagerungsbild und Zielimage\n",
    "    \n",
    "Fazit :\n",
    "- den alpha-Wert (Transparenz) des Überlagerungsbilds und den invertierten alpha-Wert (1 - mask_image) des Zielimages verwenden, um die richtigen Bildwerte zu berechnen\n",
    "- das Überlagerungsbild und Zielimage mischen, entsprechend der Transparenz, um das Ergebnis-Image zu erstellen\n",
    "    - mask_image = 1 : vollständig undurchsichtig, also das Überlagerungsbild unverändert verwenden\n",
    "    - mask_image = 0 : vollständig transparent, also das Zielimage unverändert verwenden\n",
    "    \n",
    "Aktueller Code :  \n",
    "image[y - h : y + h, x - w : x + w, c] = (overlay_image[:, :, c] * mask_image) + (image[y - h : y + h, x - w : x + w, c] * (1 - mask_image))  \n",
    "- Ergebnis-Image 3-Kanal : image[y - h : y + h, x - w : x + w, c]\n",
    "- BGR aus Überlagerungsbild 4-Kanal (overlay_image) : overlay_image[:, :, c]\n",
    "- mask_image = alpha / 255\n",
    "- Zielimage 3-Kanal : image[y - h : y + h, x - w : x + w, c]\n",
    "- Für das Zielimage werden nur die Bereiche für height, width, channel verwendet. Vom Überlagerungsbild (overlay_image) werden nur die BGR-Kanäle verwendet, nicht den alpha-Kanal (A)\n",
    "- Die Schleife for c in range(0, 3) entfernt automatisch den alpha-Kanal (c = 0, 1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c79c5e",
   "metadata": {},
   "source": [
    "Referenz: https://opencv-python.readthedocs.io/en/latest/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
